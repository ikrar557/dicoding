# -*- coding: utf-8 -*-
"""goodbooks_nb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DNbaxRqzY0XS7sb0DW4dTYVU7tRWz4HF

# System Recommendation - Good Books 10k

## Tentang Dataset dan Submission

Dataset ini berisi peringkat untuk sepuluh ribu buku populer. Secara umum, ada 100 ulasan untuk setiap buku, meskipun beberapa buku hanya memiliki sedikit ulasan.
Submission ini adalah sistem untuk merekomendasikan buku kepada user dengan fitur yang sudah ditentukan untuk `Content-Based` yaitu `title` dan `authors`, sedangkan untuk `Collaborative` menggunakan fitur `book_id` dan `rating_id`

## 1. Import Libraries

Pada tahapan ini akan di import semua library yang digunakan dalam project system rekomendasi ini.
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split

import tensorflow as tenflow
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Add
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam


import warnings
warnings.filterwarnings("ignore")

"""## 2. Data Understanding

### 2.1 Data Loading

Pada tahapan ini, dataset `books` dan `ratings` dimuat untuk selanjutnya akan dilakukan pembangunan sistem rekomendasi.
"""

books = pd.read_csv('data/books.csv', encoding = "ISO-8859-1")
ratings = pd.read_csv('data/ratings.csv')

"""Menampilkan 5 data teratas dan terbawah untuk dataset `books`"""

books

"""Menampilkan 5 data teratas dan terbawah untuk dataset `ratings`"""

ratings

"""Potongan kode ini untuk menampilkan informasi jumlah data pada dataset `books`, dan juga informasi jumlah data pada dataset `ratings`"""

print(f'Jumlah data buku adalah --> {books["book_id"].nunique()}')

print('-'*80)

print(f'Jumlah data rating keseluruhan --> {len(ratings)}')
print(f'Jumlah pengguna yang memberikan rating pada buku --> {ratings["user_id"].nunique()}')
print(f'Jumlah buku yang memiliki rating --> {ratings["book_id"].nunique()}')

"""### 2.2 Univarite Analysis

Potongan code ini untuk menampilkan jumlah baris dan kolom, beserta menampilkan kolom-kolom yang ada pada dataset `books`

#### 2.2.1 Books
"""

print("Books shape:", books.shape)
print("Books columns:", books.columns)

"""Potongan code dibawah berguna untuk menampilkan informasi dataset secara lengkap, mulai dari kolom, hingga tipe data"""

books.info()

"""Menampilkan jumlah data yang hilang untuk setiap kolom pada dataset `books`"""

books.isnull().sum()

"""Melihat Informasi statistik dari dataset `books`"""

books.describe().transpose()

"""Menampilkan distribusi rating untuk setiap buku, terlihat mayoritas buku memiliki rating di angka 4"""

plt.figure(figsize=(8,6))
sns.histplot(books['average_rating'], bins=30, kde=True)
plt.title('Distribusi Average Rating Buku')
plt.xlabel('Average Rating')
plt.ylabel('Jumlah Buku')
plt.show()

"""Menampilkan 10 penulis dengan jumlah buku terbanyak yang sudah di tulis"""

authors_count = books['authors'].value_counts()
plt.figure(figsize=(8,6))
authors_count.head(10).plot(kind='bar')
plt.title('10 Penulis Terpopuler')
plt.xlabel('Penulis')
plt.ylabel('Jumlah Buku')
plt.show()

"""#### 2.2.2 Ratings

Potongan code ini untuk menampilkan jumlah baris dan kolom, beserta menampilkan kolom-kolom yang ada pada dataset `ratings`
"""

print("Ratings shape:", ratings.shape)
print("Ratings columns:", ratings.columns)

"""Potongan code dibawah berguna untuk menampilkan informasi dataset secara lengkap, mulai dari kolom, hingga tipe data"""

ratings.info()

"""Menampilkan jumlah data yang hilang untuk setiap kolom pada dataset `ratings`"""

ratings.isnull().sum()

"""Menampilkan jumlah data yang hilang untuk setiap kolom pada dataset `ratings`"""

ratings.describe().transpose()

"""Menampilkan distribusi rating yang diberikan oleh user, terlihat rating 4 memiliki nilai paling banyak dibanding rating lainnay"""

plt.figure(figsize=(8,6))
sns.countplot(x='rating', data=ratings)
plt.title('Distribusi Rating dari Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

"""Menampilkan informasi seberapa banyak user memberi rating untuk buku-buku, kebanyakan user hanya memberikan rating 1 s.d 5x saja"""

user_rating_count = ratings['user_id'].value_counts()
plt.figure(figsize=(8,6))
sns.histplot(user_rating_count, bins=50, kde=True)
plt.title('Distribusi Jumlah Rating per Pengguna')
plt.xlabel('Jumlah Rating')
plt.ylabel('Jumlah Pengguna')
plt.show()

"""## 3. Data Preparation

Pada tahapan pembersihan dataset `books` dan `ratings` dilakukan untuk memastikan data bersih sebelum dilakukan tahapan `modeling`

### 3.1 Books

Membersihkan dataset dengan menghapus kolom yang tidak relevan, mengisi data kosong pada `language_code` dengan mengisi modus, dan sebagainya.
"""

books = books.dropna(subset=['original_title'])

books = books.drop(columns=['isbn', 'isbn13'])

books['original_publication_year'].fillna(books['original_publication_year'].median(), inplace=True)

books['language_code'] = books['language_code'].fillna(books['language_code'].mode()[0])

books['image_url'] = books['image_url'].fillna('No Image')

"""Menampilkan informasi dataset `books` setelah dilakukan pembersihan"""

print(books.isnull().sum())

"""### 3.2 Ratings

Menormalisasi feature rating sehingga model deep learning dapat lebih efektif dalam mendeteksi pola, dan juga dapat mempercepat pelatihan.
"""

min_rating = ratings['rating'].min()
max_rating = ratings['rating'].max()

ratings['rating_norm'] = (ratings['rating'] - min_rating) / (max_rating - min_rating)

"""Proses dibawah ini adalah mapping atau memetakan nilai unik dari kolom user_id dan book_id ke indeks numerik yang lebih sederhana."""

user_mapping = {id:i for i, id in enumerate(ratings['user_id'].unique())}
book_mapping = {id:i for i, id in enumerate(ratings['book_id'].unique())}

ratings['user'] = ratings['user_id'].map(user_mapping)
ratings['book'] = ratings['book_id'].map(book_mapping)

"""Memisahkan data dengan proporsi 80/10/10, dengan begini model dapat dilatih dengan data yang dicukup, dan dievalusasi, dan diuji menggunakan data yang terpisah dari data pelatihan"""

train_data, temp_data = train_test_split(ratings, test_size=0.2, random_state=42)
validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)

print(f"Jumlah data untuk pelatihan: {len(train_data)}")
print(f"Jumlah data untuk validasi: {len(validation_data)}")
print(f"Jumlah data untuk pengujian: {len(test_data)}")

X_train = [train_data['user_id'].values, train_data['book_id'].values]
y_train = train_data['rating_norm'].values

X_val = [validation_data['user_id'].values, validation_data['book_id'].values]
y_val = validation_data['rating_norm'].values

"""## 4. Modeling

### 4.1 Content-Based Filtering

Potongan code dibawah adalah menentukan fitur yang relevan untuk `modelling` pada `Content-Based`, yaitu fitur `title` dan `authors`, jadi rekomendasi akan diberikan berdasarkan `judul` dan `authors`. Sayangnya dataset ini tidak memiliki `genre`, yang mungkin akan lebih baik untuk model
"""

books['title_authors'] = books['title'] + ' ' + books['authors']

indices = pd.Series(books.index, index=books['title'])

"""Melakukan vectorizer terhadap fitur yang sudah ditentukan sebelumnya, serta menghitung similarity menggunakan Cosine Similarity untuk menghitung seberapa mirip satu data dengan data lainnya."""

tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=1, stop_words='english')
tfidf_matrix = tf.fit_transform(books['title_authors'])

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
cosine_sim

cosine_df = pd.DataFrame(cosine_sim, columns=books['title_authors'], index=books['title_authors'])
cosine_df.sample(5)

"""Potongan code dibawah adalah fungsi untuk menghasilkan rekomendasi buku berdasarkan inputan dari user, dengan menampilkan 5 buku teratas dengan nilai similarity tertinggi"""

def book_recommendations(title, n=5):
    idx = indices[title]

    sim_scores = list(enumerate(cosine_sim[idx]))

    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:n+1]

    book_indices = [i[0] for i in sim_scores]

    return books.iloc[book_indices][['book_id', 'title', 'authors', 'original_publication_year', 'average_rating']]

rekomendasi = book_recommendations('The Hobbit', 5)
rekomendasi

"""### 4.2 Collaborative Filtering

Menyesun strukur model deep learning untuk pembangunan model pendekatan `collaborative-filtering`
"""

embedding_size = 50

user_input = Input(shape=(1,), name='user_input')
book_input = Input(shape=(1,), name='book_input')

user_embedding = Embedding(
    input_dim=ratings['user_id'].nunique() + 1,
    output_dim=embedding_size,
    embeddings_regularizer=l2(1e-6)
)(user_input)

book_embedding = Embedding(
    input_dim=ratings['book_id'].nunique() + 1,
    output_dim=embedding_size,
    embeddings_regularizer=l2(1e-6)
)(book_input)

user_bias = Embedding(
    input_dim=ratings['user_id'].nunique() + 1,
    output_dim=1
)(user_input)

book_bias = Embedding(
    input_dim=ratings['book_id'].nunique() + 1,
    output_dim=1
)(book_input)

user_vec = Flatten()(user_embedding)
book_vec = Flatten()(book_embedding)

user_b = Flatten()(user_bias)
book_b = Flatten()(book_bias)

dot_product = Dot(axes=1)([user_vec, book_vec])
output = Add()([dot_product, user_b, book_b])

model = Model(inputs=[user_input, book_input], outputs=output)

model.compile(
    loss=tenflow.keras.losses.MeanSquaredError(),
    optimizer=Adam(learning_rate=0.0005),
    metrics=['mean_absolute_error', "root_mean_squared_error"])

model.summary()

"""Pembuatan `callbacks` yang berfungsi agar pelatihan dapat berjalan dengan efisiensi, termasuk waktu dan resources, dimana pelatihan dapat menyesuaikan atau berhenti secara otomatis berdasarkan `callbacks` yang dibuat."""

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=2,
    min_lr=1e-5,
    verbose=1
)

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=2,
    restore_best_weights=True,
    verbose=1
)

"""Melatih model yang sebelumnya sudah di buat sebelumnya, dengan menambahkan parameter `callbacks`"""

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10,
    callbacks=[reduce_lr, early_stop],
    batch_size=128
)

"""Melakukan pengujian terhadap model `collaborative-based` yang sudah dibangun sebelumnya"""

user_id = ratings['user_id'].sample(1).iloc[0]
user_rated_books = ratings[ratings['user_id'] == user_id]

books_not_rated_by_user = books[~books['book_id'].isin(user_rated_books['book_id'].values)]['book_id']
books_not_rated_by_user = list(books_not_rated_by_user)

user_encoded = user_id
book_encoded = books_not_rated_by_user

user_input_array = np.array([user_encoded] * len(books_not_rated_by_user))
book_input_array = np.array(books_not_rated_by_user)

predicted_ratings = model.predict([user_input_array, book_input_array]).flatten()

top_ratings_indices = predicted_ratings.argsort()[-10:][::-1]
recommended_book_ids = [books_not_rated_by_user[x] for x in top_ratings_indices]

print('Showing recommendations for user: {}'.format(user_id))
print('=' * 30)
print('Top 5 Books the user has rated:')
print('-' * 30)

top_books_user = user_rated_books.sort_values(by='rating', ascending=False).head(5).book_id.values

top_books_info = books[books['book_id'].isin(top_books_user)]
for row in top_books_info.itertuples():
    print(row.title, ':', row.authors)

print('-' * 30)
print('Top 10 Book Recommendations:')
print('-' * 30)

recommended_books = books[books['book_id'].isin(recommended_book_ids)]
for row in recommended_books.itertuples():
    print(row.title, ':', row.authors)

"""## 5. Evaluation

Evaluasi kinerja sistem rekomendasi dilakukan untuk mengukur seberapa baik sistem dalam memberikan rekomendasi yang relevan dan sesuai. Adapaun Metrik yang digunakan dalam menilai kinerja sistem rekomendasi `content-based` adalah Precision.

### 5.1 Content Based Filtering
"""

def cocok_dengan_penulis(rekomendasi, judul_buku):
    buku_asli = books[books['title'] == judul_buku]
    penulis_asli = buku_asli['authors'].values[0].split(", ")

    rekomendasi['hasil'] = rekomendasi['authors'].apply(
        lambda x: any(penulis in x for penulis in penulis_asli)
    )

    rekomendasi['persentase'] = rekomendasi['hasil'].apply(
        lambda x: 100 if x else 0
    )

    return rekomendasi

rekomendasi = book_recommendations('The Hobbit', 10)
rekomendasi_evaluasi = cocok_dengan_penulis(rekomendasi, 'The Hobbit')

jumlah_sesuai = rekomendasi_evaluasi['hasil'].sum()
jumlah_direkomendasikan = rekomendasi_evaluasi.shape[0]

presisi = jumlah_sesuai / jumlah_direkomendasikan * 100

print(f"Precision: {presisi:.2f}%")
rekomendasi_evaluasi[['title', 'authors', 'hasil', 'persentase']]

"""### 5.2. Collaborative Filtering

Evaluasi kinerja sistem rekomendasi dilakukan untuk mengukur seberapa baik sistem dalam memberikan rekomendasi yang relevan dan sesuai. Adapun metrik evaluasi yang digunakan dalam Collaborative Filtering adalah `Mean Absolute Error (MAE)` dan `Root Mean Squared Error (RMSE)`.

- Untuk perhitungannya semakin kecil nilai MAE atau RMSE maka semakin baik kemampuan sistem dalam memprediksi rating pengguna.
"""

test_user_input = np.array(test_data['user_id'].values, dtype=np.int32)
test_book_input = np.array(test_data['book_id'].values, dtype=np.int32)
test_rating = np.array(test_data['rating_norm'].values, dtype=np.float32)

loss, mae, rmse = model.evaluate([test_user_input, test_book_input], test_rating)

print(f"Loss: {loss}")
print(f"MAE: {mae}")
print(f"RMSE: {rmse}")

history_dict = history.history

loss = history_dict['loss']
val_loss = history_dict['val_loss']
mae = history_dict['mean_absolute_error']
val_mae = history_dict['val_mean_absolute_error']

rmse = np.sqrt(loss)
val_rmse = np.sqrt(val_loss)

epochs = range(1, len(loss) + 1)

plt.figure(figsize=(16, 5))

# Plot MAE
plt.subplot(1, 3, 2)
plt.plot(epochs, mae, 'bo-', label='Training MAE')
plt.plot(epochs, val_mae, 'ro-', label='Validation MAE')
plt.title('Training and Validation MAE')
plt.xlabel('Epochs')
plt.ylabel('MAE')
plt.legend()

# Plot RMSE
plt.subplot(1, 3, 3)
plt.plot(epochs, rmse, 'bo-', label='Training RMSE')
plt.plot(epochs, val_rmse, 'ro-', label='Validation RMSE')
plt.title('Training and Validation RMSE')
plt.xlabel('Epochs')
plt.ylabel('RMSE')
plt.legend()

plt.tight_layout()
plt.show()