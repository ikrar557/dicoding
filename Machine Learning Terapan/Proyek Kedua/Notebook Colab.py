# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MVUgGzoW9XKy6cqI6JzwmX9rK7jk-K2k

# System Recommendation - Good Books 10k

## 1. Import Libraries
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity, linear_kernel
from sklearn.model_selection import train_test_split

import tensorflow as tenflow
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Add, Dense
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras import layers, regularizers
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam

import plotly.graph_objects as go

import scipy.stats as stats

import warnings
warnings.filterwarnings("ignore")

"""## 2. Data Understanding

### 2.1 Data Loading
"""

books = pd.read_csv('data/books.csv', encoding = "ISO-8859-1")
ratings = pd.read_csv('data/ratings.csv')

books

ratings

print(f'Jumlah data buku adalah --> {books["book_id"].nunique()}')

print('-'*80)

print(f'Jumlah data rating keseluruhan --> {len(ratings)}')
print(f'Jumlah pengguna yang memberikan rating pada buku --> {ratings["user_id"].nunique()}')
print(f'Jumlah buku yang memiliki rating --> {ratings["book_id"].nunique()}')

"""### 2.2 Univarite Analysis

#### 2.2.1 Books
"""

print("Books shape:", books.shape)
print("Books columns:", books.columns)

books.info()

books.isnull().sum()

books.describe().transpose()

plt.figure(figsize=(8,6))
sns.histplot(books['average_rating'], bins=30, kde=True)
plt.title('Distribusi Average Rating Buku')
plt.xlabel('Average Rating')
plt.ylabel('Jumlah Buku')
plt.show()

authors_count = books['authors'].value_counts()
plt.figure(figsize=(8,6))
authors_count.head(10).plot(kind='bar')
plt.title('10 Penulis Terpopuler')
plt.xlabel('Penulis')
plt.ylabel('Jumlah Buku')
plt.show()

"""#### 2.2.2 Ratings"""

print("Ratings shape:", ratings.shape)
print("Ratings columns:", ratings.columns)

ratings.info()

ratings.isnull().sum()

ratings.describe().transpose()

plt.figure(figsize=(8,6))
sns.countplot(x='rating', data=ratings)
plt.title('Distribusi Rating dari Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

user_rating_count = ratings['user_id'].value_counts()
plt.figure(figsize=(8,6))
sns.histplot(user_rating_count, bins=50, kde=True)
plt.title('Distribusi Jumlah Rating per Pengguna')
plt.xlabel('Jumlah Rating')
plt.ylabel('Jumlah Pengguna')
plt.show()

"""## 3. Data Preparation

### 3.1 Books
"""

books = books.dropna(subset=['original_title'])

books = books.drop(columns=['isbn', 'isbn13'])

books['original_publication_year'].fillna(books['original_publication_year'].median(), inplace=True)

books['language_code'] = books['language_code'].fillna(books['language_code'].mode()[0])

books['image_url'] = books['image_url'].fillna('No Image')

print(books.isnull().sum())

"""### 3.2 Ratings"""

ratings_filtered = ratings.merge(books[['book_id', 'original_title']], on='book_id', how='inner')

print(ratings_filtered.head())

rating_counts = ratings_filtered['book_id'].value_counts()

books_with_enough_ratings = rating_counts[rating_counts > 10].index

ratings_filtered = ratings_filtered[ratings_filtered['book_id'].isin(books_with_enough_ratings)]

print(ratings_filtered['book_id'].nunique(), "unique books left after filtering")

user_rating_counts = ratings_filtered['user_id'].value_counts()

users_with_enough_ratings = user_rating_counts[user_rating_counts > 5].index

ratings_filtered = ratings_filtered[ratings_filtered['user_id'].isin(users_with_enough_ratings)]

print(ratings_filtered['user_id'].nunique(), "unique users left after filtering")

min_rating = ratings['rating'].min()
max_rating = ratings['rating'].max()

ratings['rating_norm'] = (ratings['rating'] - min_rating) / (max_rating - min_rating)

user_mapping = {id:i for i, id in enumerate(ratings['user_id'].unique())}
book_mapping = {id:i for i, id in enumerate(ratings['book_id'].unique())}

ratings['user'] = ratings['user_id'].map(user_mapping)
ratings['book'] = ratings['book_id'].map(book_mapping)

train_data, temp_data = train_test_split(ratings, test_size=0.2, random_state=42)
validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)

print(f"Jumlah data untuk pelatihan: {len(train_data)}")
print(f"Jumlah data untuk validasi: {len(validation_data)}")
print(f"Jumlah data untuk pengujian: {len(test_data)}")

X_train = [train_data['user_id'].values, train_data['book_id'].values]
y_train = train_data['rating_norm'].values

X_val = [validation_data['user_id'].values, validation_data['book_id'].values]
y_val = validation_data['rating_norm'].values

"""## 4. Modeling

### 4.1 Content-Based Filtering
"""

books['title_authors'] = books['title'] + ' ' + books['authors']

indices = pd.Series(books.index, index=books['title'])

tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=1, stop_words='english')
tfidf_matrix = tf.fit_transform(books['title_authors'])

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
cosine_sim

def book_recommendations(title, n=5):
    idx = indices[title]

    sim_scores = list(enumerate(cosine_sim[idx]))

    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:n+1]

    book_indices = [i[0] for i in sim_scores]

    return books.iloc[book_indices][['book_id', 'title', 'authors', 'original_publication_year', 'average_rating']]

rekomendasi = book_recommendations('The Hobbit', 5)
rekomendasi

"""### 4.2 Collaborative Filtering"""

embedding_size = 50

user_input = Input(shape=(1,), name='user_input')
book_input = Input(shape=(1,), name='book_input')

user_embedding = Embedding(
    input_dim=ratings['user_id'].nunique() + 1,
    output_dim=embedding_size,
    embeddings_regularizer=l2(1e-6)
)(user_input)

book_embedding = Embedding(
    input_dim=ratings['book_id'].nunique() + 1,
    output_dim=embedding_size,
    embeddings_regularizer=l2(1e-6)
)(book_input)

user_bias = Embedding(
    input_dim=ratings['user_id'].nunique() + 1,
    output_dim=1
)(user_input)

book_bias = Embedding(
    input_dim=ratings['book_id'].nunique() + 1,
    output_dim=1
)(book_input)

user_vec = Flatten()(user_embedding)
book_vec = Flatten()(book_embedding)

user_b = Flatten()(user_bias)
book_b = Flatten()(book_bias)

dot_product = Dot(axes=1)([user_vec, book_vec])
output = Add()([dot_product, user_b, book_b])

model = Model(inputs=[user_input, book_input], outputs=output)

model.compile(
    loss=tenflow.keras.losses.MeanSquaredError(),
    optimizer=Adam(learning_rate=0.0005),
    metrics=['mean_absolute_error', "root_mean_squared_error"])

model.summary()

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=2,
    min_lr=1e-5,
    verbose=1
)

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=2,
    restore_best_weights=True,
    verbose=1
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10,
    callbacks=[reduce_lr, early_stop],
    batch_size=128
)

user_id = ratings['user_id'].sample(1).iloc[0]
user_rated_books = ratings[ratings['user_id'] == user_id]

books_not_rated_by_user = books[~books['book_id'].isin(user_rated_books['book_id'].values)]['book_id']
books_not_rated_by_user = list(books_not_rated_by_user)

user_encoded = user_id
book_encoded = books_not_rated_by_user

user_input_array = np.array([user_encoded] * len(books_not_rated_by_user))
book_input_array = np.array(books_not_rated_by_user)

predicted_ratings = model.predict([user_input_array, book_input_array]).flatten()

top_ratings_indices = predicted_ratings.argsort()[-10:][::-1]
recommended_book_ids = [books_not_rated_by_user[x] for x in top_ratings_indices]

print('Showing recommendations for user: {}'.format(user_id))
print('=' * 30)
print('Top 5 Books the user has rated:')
print('-' * 30)

top_books_user = user_rated_books.sort_values(by='rating', ascending=False).head(5).book_id.values

top_books_info = books[books['book_id'].isin(top_books_user)]
for row in top_books_info.itertuples():
    print(row.title, ':', row.authors)

print('-' * 30)
print('Top 10 Book Recommendations:')
print('-' * 30)

recommended_books = books[books['book_id'].isin(recommended_book_ids)]
for row in recommended_books.itertuples():
    print(row.title, ':', row.authors)

"""## 5. Evaluation

### 4.1 Content Based Filtering
"""

def cocok_dengan_penulis(rekomendasi, judul_buku):
    buku_asli = books[books['title'] == judul_buku]
    penulis_asli = buku_asli['authors'].values[0].split(", ")

    rekomendasi['hasil'] = rekomendasi['authors'].apply(
        lambda x: any(penulis in x for penulis in penulis_asli)
    )

    rekomendasi['persentase'] = rekomendasi['hasil'].apply(
        lambda x: 100 if x else 0
    )

    return rekomendasi

rekomendasi = book_recommendations('The Hobbit', 10)
rekomendasi_evaluasi = cocok_dengan_penulis(rekomendasi, 'The Hobbit')

jumlah_sesuai = rekomendasi_evaluasi['hasil'].sum()
jumlah_direkomendasikan = rekomendasi_evaluasi.shape[0]

presisi = jumlah_sesuai / jumlah_direkomendasikan * 100

print(f"Precision: {presisi:.2f}%")
rekomendasi_evaluasi[['title', 'authors', 'hasil', 'persentase']]

test_user_input = np.array(test_data['user_id'].values, dtype=np.int32)
test_book_input = np.array(test_data['book_id'].values, dtype=np.int32)
test_rating = np.array(test_data['rating_norm'].values, dtype=np.float32)

loss, mae, rmse = model.evaluate([test_user_input, test_book_input], test_rating)

print(f"Loss: {loss}")
print(f"MAE: {mae}")
print(f"RMSE: {rmse}")

history_dict = history.history

loss = history_dict['loss']
val_loss = history_dict['val_loss']
mae = history_dict['mean_absolute_error']
val_mae = history_dict['val_mean_absolute_error']

rmse = np.sqrt(loss)
val_rmse = np.sqrt(val_loss)

epochs = range(1, len(loss) + 1)

plt.figure(figsize=(16, 5))

plt.subplot(1, 3, 1)
plt.plot(epochs, loss, 'bo-', label='Training Loss (MSE)')
plt.plot(epochs, val_loss, 'ro-', label='Validation Loss (MSE)')
plt.title('Training and Validation Loss (MSE)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot MAE
plt.subplot(1, 3, 2)
plt.plot(epochs, mae, 'bo-', label='Training MAE')
plt.plot(epochs, val_mae, 'ro-', label='Validation MAE')
plt.title('Training and Validation MAE')
plt.xlabel('Epochs')
plt.ylabel('MAE')
plt.legend()

# Plot RMSE
plt.subplot(1, 3, 3)
plt.plot(epochs, rmse, 'bo-', label='Training RMSE')
plt.plot(epochs, val_rmse, 'ro-', label='Validation RMSE')
plt.title('Training and Validation RMSE')
plt.xlabel('Epochs')
plt.ylabel('RMSE')
plt.legend()

plt.tight_layout()
plt.show()